---
layout: post
title: ! 'Issue #3'
author: ceberhardt
---

## [AI Coding Agents Are Removing Programming Language Barriers](https://railsatscale.com/2025-07-19-ai-coding-agents-are-removing-programming-language-barriers/)

<small>RAILSATSCALE.COM</small>

I'm always interested in articles where experienced engineers share practical 'lived' examples of how AI has augmented their abilities, and this one is another great example. 

Stan is a career Ruby dev, with a decade of experience and a great depth of knowledge. Recently, for a variety of reasons, Stan has been pushed out of his comfort zone, having to pick up C, Rust and a host of low-level tasks. 

Stan reports that:

> The real breakthrough came when I stopped thinking of AI as a code generator and started treating it as a pairing partner with complementary skills.

They go on to describe a simple breakdown of how this pair programming work in practice. A really interesting read.

## ðŸ“¹ [Does AI Actually Boost Developer Productivity?](https://www.youtube.com/watch?v=tbDDYKRFjhk)

<small>YOUTUBE.COM</small>

The hype around AI-accelerated productivity continues to climb, with the frankly ridiculous claim from Surge CEO that [AI is creating 100x engineers](https://www.businessinsider.com/surge-ceo-ai-100x-engineers-2025-7). Finding thoughtful, balanced and accurate measurements of AIs impact is not easy.

This talk, from Yegor Denisov (Stanford) caught my attention. They have measured the productivity, across a range of 'enterprise' tasks, in 100s of teams.

![developer productivity]({{"img/3.png"| absolute_url}})

I want to just blindly agree with this study, because the results roughly match my intuition, i.e. greenfield is accelerated from 12%-31%, brownfield from %-16% However, there are a few aspects of this experiment that Iâ€™d like to know more about.

Automated evaluation of code quality (alongside other quantitative metrics) is a tricky subject, which they appear to have â€˜solvedâ€™ in order to undertake this analysis at scale. 

I certainly don't have any reason to doubt there work, but I would like to understand more about their methodology and its potential limitations.


## [How Anthropic teams use Claude Code](https://www.anthropic.com/news/how-anthropic-teams-use-claude-code)

<small>ANTHROPIC.COM</small>

This case study is a little length, and it isn't much more than post-it note level notes. However, it is still interesting to hear from Anthropic, who tend to avoid the AI hype of their competitors. It's a good SmÃ¶rgÃ¥sbord of ideas.

## [My 2.5 year old laptop can write Space Invaders in JavaScript now](https://simonwillison.net/2025/Jul/29/space-invaders/)

<small>SIMONWILLISON.NET</small

For the vast majority of us we are using 'cloud based' AI models (GPT, Claude, etc), simply because it is easy and the costs are (currently) quite reasonable. However, there are a number of advantages to running a code model locally; it is more secure (your code isn't being sent to a third party), it can be more reliable and the costs is much lower.

In this blog post Simon describes his experiences with GLM-4.5, released as an open weights model by the Chinese Z.ai lab. Given that it is open source, it has allowed the community to create a quantized (i.e. compressed) version that works very well on a modest laptop. Simon concludes that:

> Local coding models are really good now

ðŸ˜ŠðŸ˜ŠðŸ˜Š


## And finally

Anthropic are [introducing monthly rate limits on Claude Code](https://news.ycombinator.com/item?id=44713757). With most of these tools currently operating at a significant loss, we're likely to see more of this over the next few months.

Time to switch to a local model perhaps?


