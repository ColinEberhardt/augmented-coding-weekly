---
layout: post
title: ! 'Issue #32'
author: ceberhardt
published: false
thumbnail:
summary: 
---

## [Are Repository-Level Context Files Helpful for Coding Agents?](https://arxiv.org/abs/2602.11988)

<small>ARXIV.ORG</small>

It is very common for developers to create `AGENTS.md` files, that provide guidance and instructions for AI agents working on their codebase. These files are supported by most popular agentic frameworks, and have become [something of an (informal) standard](https://agents.md/), with around 60k `AGENTS.md` files found on GitHub.

Furthermore, writing agent guidance is a bit of a chore, so developers will often ask the agent to generate this file for them, based on common conventions and guidance around what make a good `AGENTS.md` file.

But do these files actually work?

I must admit, I have been a long-time skeptic of just copy-pasting someone elses prompt into my workflow, how can I be sure it actually adds value and doesn't just waste tokens. As models become ever-more capable, I'm finding that I need to spend much less time on prompting.

This study attempts to answer the question of whether `AGENTS.md` scientifically, via a benchmark.

They find that human-authored files create a 4% improvement in performance, while LLM-generated equivalents results in a reduce performance, by 3%. Not a great result. Worse still ...

> "we observe that context files lead to increased exploration, testing, and reasoning by coding agents, and, as a result, increase costs by over 20%."

As a result, they recommend that you omitting LLM-generated context files, and that human authored ones are intentionally minimal.

My personal approach is to start with nothing, no `AGENTS.md` at all. I only add it when I observe a need to encourage a specific behaviour.