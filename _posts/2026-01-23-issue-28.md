---
layout: post
title: ! 'Issue #28'
author: ceberhardt
published: false
thumbnail: 
summary: 
---

## [Crypto grifters are recruiting open-source AI developers](https://www.seangoedecke.com/gas-and-ralph/)

<small>SEANGOEDECKE.COM</small>

This story is a strange one ...

A couple of the more 'out there' AI engineering projects to emerge recently are Geoff Huntley’s “Ralph Wiggum loop” (giving Claude code infinite context by running in a never ending loop) and Steve Yegge’s “Gas Town” (a whole village of LLM workers churning out code at speed). They might not be the most practical projects, but they are certainly generating discussion and more than a little bit of hype. 

I had this to say of Gas Town a [few issues back](https://augmentedcoding.dev/issue-26/):

> Personally I think of Gas Town as a work of modern art, it is a provocation rather than a solution.

However, since then both Huntley and Yegge have been posting about $RALPH and $GAS cryptocurrency coins (meme coins). What on earth is going on?

The Solana network has an app called Bags where you can create new meme coins, with a cut of the profit going to a nominated Twitter (X) account. SOmeone created meme coins for each of these projects, with the payout for $GAS totalling $300k at the moment.

This is a complicated issue - for most people open source doesn't pay, so having someone suddenly appear with a considerable bag of money is an enticing proposition. However, this is very much predatory behaviour on the part of the cryto grifters. Yes, Huntley and Yegge gain some funds, but they are then incentivised to increase this by promoting their respective meme coins, and the more people who buy them, the more money the grifters make, they will always ensure they get the lion's share of the reward. 

Just as art attracts NFTs, open source is now attracting memecoins. 

## [Cursor's latest "browser experiment" implied success without evidence](https://embedding-shapes.github.io/cursor-implied-success-without-evidence/)

<small>GITHUB.IO</small>

The current cohort of frontier models (Claude, GPT, etc) all have very similar performance across a wide range of benchmarks, as a result, there seems to be a new way to compare performance - their ability to operate autonomously for long periods of time. There's even a benchmark for this, [developed and run by METR](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/). 

Recent announcements cite models working for hours on complex tasks, Cursor have upped the ante - moving to weeks!

A few days back the Cursor team published a blog, [Scaling long-running autonomous coding](https://cursor.com/blog/scaling-agents), where they described their work in running a fleet of autonomous agents for weeks in order to build a highly complex application, a web browser. They shared the project repo, with 1,000 of files and more than a million lines of code.

It's impressive how much code they generated in such a short space of time. 

However, there's a subtle issue here. The Cursor blog post implies this was a great success, but never states that the browser actually worked. Unfortunately it didn't. This blog post picks apart the codebase, finding that it doesn't compile, and is a rather disappointing mess.

Another example of _"hype first and context later"_