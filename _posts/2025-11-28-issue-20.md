---
layout: post
title: ! 'Issue #20'
author: ceberhardt
published: false
thumbnail: img/20.png
summary: 
---


## [Introducing Claude Opus 4.5](https://www.anthropic.com/news/claude-opus-4-5)

<small>ANTHROPIC.COM</small>

It's been an exciting few weeks for model releases, with recent foundation model releases all having a strong focus on autonomous AI coding.

* 18th Nov, Google released Gemini 3.0, with leading results across almost every benchmark (SWE-Bench being the notable exception). They also released Antigravity, their AI-first IDE.
* 19th Nov, OpenAI released GPT-5 Codex Max, trained on agentic tasks across software engineering, math, research - with a focus on speed and efficiency
* 24th Nov, Anthropic released Claude Opus 4.5, achieving substantial improvements in complex code generation, autonomous agents, enterprise tasks, and long-running workflows

Earlier this year there was a lot of talk about models hitting the scaling laws, due to limitations in data and training time. While it is true that there are limits to what can be achieved through training alone, this year we've seen a lot of innovation in post-training activities; tools and computer use, improved reasoning and efficiency.

As a result, benchmark scores continue to improve at an impressive rate. Notably Opus 4.5 has now hit >80% pass rate on [SWE-Bench](https://www.swebench.com/), resulting in the creation of a newer and harder [SWE-Bench Pro](https://scale.com/leaderboard/swe_bench_pro_public).

## [Putting Spec Kit Through Its Paces: Radical Idea or Reinvented Waterfall?](https://blog.scottlogic.com/2025/11/26/putting-spec-kit-through-its-paces-radical-idea-or-reinvented-waterfall.html)

<small>SCOTTLOGIC.COM</small>

I recently put Spec-Driven Development (SDD) to the test by rebuilding a feature in my hobby app using GitHub’s Spec Kit. What I found was surprising: despite the promise of clean specifications and structured AI workflows, the real-world experience was slow, heavy, and far less effective than the lightweight iterative approach I normally use with AI coding agents.

![SDD]({{"img/20.png"| absolute_url}})

In this post, I break down the experiment, share the data, and explore where SDD shines, where it struggles, and what this might mean for how we build software in the age of AI. If you’re curious whether SDD is the future—or just a fascinating detour—you might find this an interesting read.

## [Building an AI-Native Engineering Team](https://developers.openai.com/codex/guides/build-ai-native-engineering-team/)

<small>OPENAI.COM</small>

AI augmented software development is much more than just writing code faster, it is about transforming the way that we approach the craft of software development itself.

Unfortunately this topic of conversation often veers into hype-fuelled nonsense!

I'm pleased to see OpenAI publishing a guide that looks at the full software lifecycle (Plan, Design, Build, ... Maintain), considering the impact agentic AI has and what Engineers now "do instead". This is a very practical way of looking at the transformative effects of AI.

Considering that OpenAI are a product company that sells AI Agents, there is a bit of overreach in some of their statements around what these coding agents are truly capable of, but the overall framework makes a lot of sense.