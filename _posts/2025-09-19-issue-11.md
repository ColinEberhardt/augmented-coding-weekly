---
layout: post
title: ! 'Issue #11'
author: ceberhardt
published: false
---

## [Getting AI to Work in Complex Codebases](https://github.com/humanlayer/advanced-context-engineering-for-coding-agents/blob/main/ace-fca.md)

<small>GITHUB.COM</small>

It is no great secret that AI tools excel at tasks that are in some way a reflection of their training dataset. However, when applying these tools to complex codebases, those that use proprietary or unusual libraries / APIs, or less popular languages, can be a challenge. Often, in these cases, the complexity of task you can delegate to your AI tool is modest. Put simply, it needs a lot of hand-holding.

In order to execute more sizeable tasks (e.g. one-shot feature development, large scale refactor), you must supply the AI tool with a lot of instructions. Although there is a limit to the amount of information you can provide to a model (due to context window token limits), especially when you consider that they are stateless (i.e. you have to provide them the complete set of instruction on each and every invocation)

Finding a workable approach that balances the need for detailed instructions, and the limited context window, is something of an art form.

In this blog post, the author outlines a structured approach to this challenge, through a process of Research → Plan → Implement, and intentional compaction of context they report some impressive result.

We're at the very early stages of working out how to make best use of the incredible power that AI tools can deliver. I think we're going to see a lot of innovation here before a consolidated and optimised approach begins to emerge.

## [CompileBench: Can AI Compile 22-year-old Code?](https://quesma.com/blog/introducing-compilebench/)

<small>QUESMA.COM</small>

And once again on the topic of applying AI to complex, messy, real-world tasks ... 

CompileBench is a new benchmark suite for evaluating agentic AI models (i.e. models that iteratively tackle complex tasks), by challenging them to perform complex and messy tasks, for example "reviving 2003-era code, cross-compiling to Windows, or cross-compiling for ARM64 architecture".

What I like about this approach is that there will likely be limited information relating to that specific task in their training dataset. And as a result, they will have to employ genuine problem solving to successfully complete the task.

![codebench]({{"img/11.png"| absolute_url}})

You can review the results to see which model currently performs the best across these gnarly problems.

Winners and losers aside, I think it is amazing that an AI agent can actually complete these tasks. It shows genuine problem solving ability. However, theses tasks are rather narrow in focus, i.e. get something to build. They are tasks that are easy to describe and easy to evaluate. 

Regardless, this is a fascinating and interesting piece of work.
