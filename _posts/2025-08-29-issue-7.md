---
layout: post
title: ! 'Issue #7'
author: ceberhardt
published: true
thumbnail: /img/7.jpg
summary: A developer's rationale for declining AI-generated merge requests and the learning value of code review, a free workshop on building coding agents with surprisingly simple architecture, enterprise challenges with vibe debugging and the need for AI-proof development pipelines, and a GitHub project requiring AI tool disclosure for contributions.
---

## [Why I'm declining your AI generated MR](https://blog.stuartspence.ca/2025-08-declining-ai-slop-mr.html)

<small>STUARTSPENCE.CA</small>

Another thoughtful blog post that grapples with the role AI has (or should have) in our industry.

Stuart reserved the right to decline your MR (Merge Request - aka Pull Request) if he considers it to be AI generated, using this page to provide his rationale. The blog post delves further into the role code review has in the process of software development, notably the opportunity for both author and reviewer to learn.

An interesting AI anti-pattern that Stuart calls out specifically is 'documentation spam'. AI tools and agents tend to create more documentation than humans, giving a surface-level impression of good code quality. Finding the right balance of code to documentation is a challenging, but most would agree that good quality code requires less documentation.

Finally Start shares:

> It's not always clear to me when it's a good use of AI that I should support with a full CR, or when it's a bad use of AI that I need to confront by rejecting it entirely.

I can relate to that. For many of us it is hard to know when you should use AI and when you should not. Hopefully we'll find our way soon.

## [how to build a coding agent: free workshop](https://ghuntley.com/agent/)

<small>GHUNTLEY.COM</small>

While coding agents sounds complicated, mystical and magical, they are actually surprisingly simple. Yes, the underlying LLM technology is all of those things (mystical and magical), but the software layer that sits on top to turn an LLM into an autonomous coding agent is surprisingly simple. This blog post, based on a recent talk, describes the architecture and process of creating an agent.

![AI agent]({{"img/7.jpg"| absolute_url}})

I did something similar a few years back, looking at how the [core functionality of langchain could be implemented in 100 lines of code](https://blog.scottlogic.com/2023/05/04/langchain-mini.html).

## [Vibe Debugging: Enterprises' Up and Coming Nightmare](https://marketsaintefficient.substack.com/p/vibe-debugging-enterprises-up-and)

<small>SUBSTACK.COM</small>

This blog post starts with a small cautionary tale, about the maintenance issues faced with a vibe coded application. Interestingly the author understands how these issues could be mitigated, but just like so many of us, got carried away with the joy of creating software at pace.

The rest of the post explores the practical realities that organisations are facing when adopting these tools. Enterprises, caught in an arms race, can’t afford to slow down adoption, but the trade-offs are becoming starkly evident.

Enterprises must rethink their entire development pipeline in response. Traditional safeguards—code reviews, testing, and CI/CD—aren’t enough when faced with exponential code growth. Instead, organizations need AI-proof pipelines with intelligent quality gates, rigorous static analysis, and near-real-time monitoring. Observability is especially critical: debugging an AI-generated codebase at 3 AM requires fast, reliable insights, not guesswork. 

This post does a good job of tempering hype with realism. AI isn’t progressing in a straight line, breakthroughs are followed by plateaus, and enterprises can’t wait for flawless models to arrive. Instead, they must build resilient systems for the imperfect tools available today. The wave of AI-augmented development is already reshaping how software is built, and success will depend less on generating code quickly and more on safeguarding it intelligently.

Wise words.

## [AI tooling must be disclosed for contributions](https://github.com/ghostty-org/ghostty/pull/8289)

<small>GITHUB.COM</small>

Ghostty is a very popular terminal emulator, with around 35k stars on GitHub. This recently documentation update adds a requirement that if you use any form of AI assistance in contributing to the project, this must be detailed and disclosed in your pull request.

The reason for this disclosure is as follows:

> "In a perfect world, AI assistance would produce equal or higher quality work than any human. That isn't the world we live in today, and in most cases it's generating slop. I say this despite being a fan of and using them successfully myself (with heavy supervision)!"

It's frustrating that a popular open source project has to make this mandate, however, this isn't the first time I've heard of open source maintainers struggling with an increase of "AI Slop". Daniel Stenberg, maintainer of curl and open source rockstar, [recently shared](https://www.linkedin.com/posts/danielstenberg_hackerone-curl-activity-7324820893862363136-glb1/) that he is being overwhelmed by poor quality contributions and CVE reports.  

Unfortunately AI does make it all too easy to create large quantities of poor quality content and code.
