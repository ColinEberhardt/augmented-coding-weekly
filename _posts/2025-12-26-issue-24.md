---
layout: post
title: ! 'Issue #24'
author: ceberhardt
published: true
thumbnail: img/24.png
summary: Dealing with AI agent variance through parallel runs, practical experiences implementing complex algorithms with agentic loops, reflections on how coding agents are changing software development, and the local vs. cloud model debate.
---

## [One Agent Isn't Enough](https://benr.build/blog/one-agent-isnt-enough)

<small>BENR.BUILD</small>

> "Agentic coding has a problem - variance. What if single-agent runs are leaving performance on the table by design?"

The goal of context and prompt engineering is to direct an AI system (such as a coding agent) to perform a task in a well-defined manner, in order to produce a correct (or acceptable) answer first time. However, AI Agents are non-deterministic systems, producing different results on each re-run, even if you supply exactly the same prompt and context. 

Your context engineering and word wrangling may produce an acceptable result, but is it the best?

![parallel runs]({{"img/24.png"| absolute_url}})

Ben's solution to this problem is to perform parallel runs, where multiple agents perform the same task, and an orchestration agent compares each solution, looking for similarity and convergence.

An interesting idea - I don't think this is an approach I'd use all the time, but for tasks that warrant creativity and exploration (e.g. refactoring), this could be a useful technique.

## [The power of agentic loops - implementing flexbox layout in 3 hours](https://blog.scottlogic.com/2025/12/22/power-of-agentic-loops.html)

<small>SCOTTLOGIC.COM</small>

I was heavily inspired by Emil's post which I covered a few weeks back, where he developed a pure-Python implementation of a HTML5 parser, [leaning heavily on coding agents](https://friendlybit.com/python/writing-justhtml-with-coding-agents/). I decided to give this approach a try, re-implementing the browser's flexbox algorithm in JavaScript.

Instead of asking AI to write perfect code in one shot, I gave it:
 - A clear goal (implement flexbox)
 - Tools to test itself (browser reference implementation)
 - Direction to iterate autonomously
 - Self improvement, via reflecting on the challenges at each step

The result: ~800 lines of code with 350 tests, all validated against browser implementations.

It was an amazing learning experience, which I'd encourage you to have a go at.

## [A Year Of Vibes](https://lucumr.pocoo.org/2025/12/22/a-year-of-vibes/)

<small>POCOO.ORG</small>

For Armin 2025 was the "year of agents" where he "stopped programming the way I did before". This blog post doesn't share much about Armin's approach, it isn't a prompting or context engineering guide - rather, it focusses on how he feels about this shift, making it a much more interesting read.

Some of his experiences really resonate, when working on the flexbox implementation (the post above) I was struck by how 'human' their thought process is. As Armin observes, these models are no longer "mere token tumblers".

There are many more thoughtful observations in this post - interestingly, many are open ended questions. 

## [You Donâ€™t Need to Spend $100/mo on Claude Code](https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude)

<small>AIFORSWES.COM</small>

This blog post explores wether the it is better to invest in a more powerful laptop and use a local model that pay a premium subscription for a cloud-based model.

As you can see from the lengthy edits and reflections at the top of this post it generated quite a bit of debate and push-back in the [hacker news](https://news.ycombinator.com/item?id=46348329) comments. Regardless, this is an interesting post that highlights just how capable local models have become.

I do think there is value in local models for their offline capability, but with models evolving at such a pace, I prefer the convenience of cloud hosting and am happy to pay a modest premium for that convenience.