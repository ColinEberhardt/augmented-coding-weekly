---
layout: post
title: ! 'Issue #3'
author: ceberhardt
published: false

---

## [AI Coding Agents Are Removing Programming Language Barriers](https://railsatscale.com/2025-07-19-ai-coding-agents-are-removing-programming-language-barriers/)

<small>RAILSATSCALE.COM</small>

I'm always interested in articles where experienced engineers share practical 'lived' examples of how AI has augmented their abilities, and this one is another great example. 

Stan is a career Ruby dev, with a decade of experience and a great depth of knowledge. Recently, for a variety of reasons, Stan has been pushed out of his comfort zone, having to pick up C, Rust and a host of low-level tasks. 

Stan reports that:

> The real breakthrough came when I stopped thinking of AI as a code generator and started treating it as a pairing partner with complementary skills.

They go on to describe a simple breakdown of how this pair programming work in practice. A really interesting read.

## ðŸ“¹ [Does AI Actually Boost Developer Productivity?](https://www.youtube.com/watch?v=tbDDYKRFjhk)

<small>YOUTUBE.COM</small>

The hype around AI-accelerated productivity continues to climb, with the frankly ridiculous claim from Surge CEO that [AI is creating 100x engineers](https://www.businessinsider.com/surge-ceo-ai-100x-engineers-2025-7). Finding thoughtful, balanced and accurate measurements of AIs impact is not easy.

This talk, from Yegor Denisov (Stanford) caught my attention. They have measured the productivity, across a range of 'enterprise' tasks, in 100s of teams.

![developer productivity]({{"img/3.png"| absolute_url}})

I want to just blindly agree with this study, because the results roughly match my intuition, i.e. greenfield is accelerated from 12%-31%, brownfield from %-16% However, there are a few aspects of this experiment that Iâ€™d like to know more about.

Automated evaluation of code quality (alongside other quantitative metrics) is a tricky subject, which they appear to have â€˜solvedâ€™ in order to undertake this analysis at scale. 

I certainly don't have any reason to doubt there work, but I would like to understand more about their methodology and its potential limitations.

## [How Anthropic teams use Claude Code](https://www.anthropic.com/news/how-anthropic-teams-use-claude-code)

<small>ANTHROPIC.COM</small>

This case study is a little length, and it isn't much more than post-it note level notes. However, it is still interesting to hear from Anthropic, who tend to avoid the AI hype of their competitors. It's a good SmÃ¶rgÃ¥sbord of ideas.

## And finally

Anthropic are [introducing monthly rate limits on Claude Code](https://news.ycombinator.com/item?id=44713757). With most of these tools currently operating at a significant loss, we're likely to see more of this over the next few months.


